{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e95cdfd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245207c653d64744b195a4bdcea98e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5920 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25868969d5da4efb8c2a6232af850f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1481 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e331eaf36272407e84b657312d338834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1851 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_56828/2050967887.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[device] cuda:NVIDIA RTX A6000 | TF32=True\n",
      "=== Eval BEFORE fine-tune ===\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8999132513999939, 'eval_model_preparation_time': 0.0058, 'eval_accuracy': 0.14652261985145174, 'eval_runtime': 0.942, 'eval_samples_per_second': 1572.154, 'eval_steps_per_second': 12.739}\n",
      "=== Training (no checkpoints / no mid-eval) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='141' max='141' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [141/141 00:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_time = 30.6s ; steps = 141\n",
      "=== Eval AFTER fine-tune ===\n",
      "{'eval_loss': 0.13750220835208893, 'eval_model_preparation_time': 0.0058, 'eval_accuracy': 0.949358541525996, 'eval_runtime': 0.991, 'eval_samples_per_second': 1494.463, 'eval_steps_per_second': 12.109, 'epoch': 3.0}\n",
      "wrote: ./outputs/submission_bert2.csv\n"
     ]
    }
   ],
   "source": [
    "# ======= server_min_io_finetune.py =======\n",
    "\n",
    "import os, pathlib\n",
    "\n",
    "# --- 1) 强制单卡 / 清理分布式变量，避免误走多卡 & P2P 映射 ---\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "for k in (\"RANK\",\"LOCAL_RANK\",\"WORLD_SIZE\",\"MASTER_ADDR\",\"MASTER_PORT\"):\n",
    "    os.environ.pop(k, None)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# --- 2) Hugging Face 缓存尽量走内存盘/本地快盘，减少慢盘 I/O ---\n",
    "ramdisk = \"/dev/shm\" if os.path.isdir(\"/dev/shm\") else \".\"\n",
    "hf_home = os.path.join(ramdisk, \".hf_cache\")\n",
    "os.makedirs(hf_home, exist_ok=True)\n",
    "os.environ[\"HF_HOME\"] = hf_home\n",
    "os.environ[\"HF_HUB_CACHE\"] = os.path.join(hf_home, \"hub\")\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = os.path.join(hf_home, \"transformers\")\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = os.path.join(hf_home, \"datasets\")\n",
    "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"   # 纯本地，少网络交互\n",
    "\n",
    "# --------------------- 下面才开始 import ---------------------\n",
    "import json, time, random\n",
    "import numpy as np, pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, disable_caching\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments, DataCollatorWithPadding\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# --- 3) CUDA 设备 & TF32（Ampere 加速） ---\n",
    "assert torch.cuda.is_available(), \"CUDA 不可用：确认驱动/容器/权限\"\n",
    "device = torch.device(\"cuda\")\n",
    "torch.set_float32_matmul_precision(\"high\")      # TF32 快路径（2.0+）:contentReference[oaicite:3]{index=3}\n",
    "torch.backends.cuda.matmul.allow_tf32 = True    # Ampere 张量核 TF32 开启:contentReference[oaicite:4]{index=4}\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# --- 4) 随机种子 ---\n",
    "SEED = 999\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# --- 5) 读入你的本地数据（/mnt/data/train.json），自动识别列名 ---\n",
    "def read_json_flexible(path: str) -> pd.DataFrame:\n",
    "    # 先按 JSON Lines 读；不行就回退到普通 JSON\n",
    "    try:\n",
    "        return pd.read_json(path, lines=True)\n",
    "    except ValueError:\n",
    "        return pd.read_json(path)\n",
    "\n",
    "DATA_PATH = \"../train.json\"\n",
    "df = read_json_flexible(DATA_PATH)\n",
    "text_key_candidates  = [\"text\",\"review\",\"reviews\",\"sentence\",\"content\"]\n",
    "label_key_candidates = [\"label\",\"labels\",\"sentiment\",\"sentiments\",\"target\",\"y\"]\n",
    "text_col  = next((c for c in text_key_candidates  if c in df.columns), None)\n",
    "label_col = next((c for c in label_key_candidates if c in df.columns), None)\n",
    "assert text_col and label_col, f\"列名识别失败，请检查字段；现有列：{df.columns.tolist()}\"\n",
    "\n",
    "df = df[[text_col, label_col]].rename(columns={text_col:\"text\", label_col:\"label\"}).copy()\n",
    "# 规范二分类标签到 {0,1}\n",
    "mapping = {\"neg\":0,\"negative\":0,\"0\":0,\"pos\":1,\"positive\":1,\"1\":1}\n",
    "if not pd.api.types.is_integer_dtype(df[\"label\"]):\n",
    "    df[\"label\"] = df[\"label\"].astype(str).str.lower().map(mapping)\n",
    "df = df.dropna(subset=[\"label\"]).astype({\"label\":\"int64\"})\n",
    "\n",
    "# 训练/验证划分（分层）\n",
    "train_df, valid_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=SEED, stratify=df[\"label\"]\n",
    ")\n",
    "test_path = \"../test.json\"\n",
    "test_df = read_json_flexible(test_path)\n",
    "text_col = next((c for c in [\"text\",\"review\",\"reviews\",\"sentence\",\"content\"] if c in test_df.columns), None)\n",
    "assert text_col, f\"测试集未找到文本列，现有列：{test_df.columns.tolist()}\"\n",
    "# --- 6) HF Datasets：禁用磁盘缓存 + 仅驻内存 tokenization ---\n",
    "disable_caching()  # 全局禁用 datasets 缓存（transform 结果不落盘）:contentReference[oaicite:5]{index=5}\n",
    "def to_hf(pdf: pd.DataFrame) -> Dataset:\n",
    "    return Dataset.from_pandas(pdf[[\"text\",\"label\"]], preserve_index=False)\n",
    "\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\", \"bert-base-uncased\") \n",
    "MAX_LEN = int(os.getenv(\"MAX_LEN\", \"128\"))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "def tok_fn(batch):  # 只做截断，padding 交给 collator\n",
    "    return tokenizer(batch[\"text\"], truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "hf_train = to_hf(train_df).map(tok_fn, batched=True, batch_size=2048,\n",
    "                               remove_columns=[\"text\"], keep_in_memory=True, load_from_cache_file=False)\n",
    "hf_valid = to_hf(valid_df).map(tok_fn, batched=True, batch_size=2048,\n",
    "                               remove_columns=[\"text\"], keep_in_memory=True, load_from_cache_file=False)\n",
    "hf_train = hf_train.rename_column(\"label\", \"labels\").with_format(\"torch\")\n",
    "hf_valid = hf_valid.rename_column(\"label\", \"labels\").with_format(\"torch\")\n",
    "\n",
    "from datasets import Dataset\n",
    "hf_test = Dataset.from_pandas(test_df[[text_col]].rename(columns={text_col:\"text\"}), preserve_index=False)\n",
    "hf_test = hf_test.map(lambda b: tokenizer(b[\"text\"], truncation=True, max_length=MAX_LEN),\n",
    "                      batched=True, batch_size=2048, remove_columns=[\"text\"],\n",
    "                      keep_in_memory=True, load_from_cache_file=False)\n",
    "\n",
    "\n",
    "# --- 7) 模型 ---\n",
    "id2label = {0:\"NEG\", 1:\"POS\"}; label2id = {\"NEG\":0, \"POS\":1}\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=2, id2label=id2label, label2id=label2id, cache_dir=os.environ[\"TRANSFORMERS_CACHE\"]\n",
    ").to(device)\n",
    "\n",
    "# --- 8) Collator：按 8 对齐 padding，利于张量核 ---\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)  # 文档支持该参数:contentReference[oaicite:6]{index=6}\n",
    "\n",
    "# --- 9) 最小 I/O 的 TrainingArguments（不存盘、不报告、不评估中间结果） ---\n",
    "from dataclasses import fields as dataclass_fields\n",
    "allowed = {f.name for f in dataclass_fields(TrainingArguments)}\n",
    "\n",
    "args_dict = dict(\n",
    "    output_dir=os.path.join(ramdisk, \"_out\"),  # 若有 /dev/shm 则写内存盘；否则当前目录\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    # ——全程不写盘、仅控制台打印——\n",
    "    save_strategy=\"no\",\n",
    "    load_best_model_at_end=False,\n",
    "    report_to=\"none\",\n",
    "    logging_strategy=\"no\",\n",
    "\n",
    "    # 训练中不评估；我们手动在训练前/后各评一次，避免 I/O\n",
    "    eval_strategy=\"no\" if \"eval_strategy\" in allowed else None,\n",
    "    evaluation_strategy=None if \"eval_strategy\" in allowed else \"no\",\n",
    "\n",
    "    # DataLoader：GPU 更友好（pin_memory/num_workers）\n",
    "    dataloader_pin_memory=True,   # PyTorch 官方推荐 GPU 下开启:contentReference[oaicite:7]{index=7}\n",
    "    dataloader_num_workers=4,     # 典型 2~8；需结合机器核数调优:contentReference[oaicite:8]{index=8}\n",
    "    group_by_length=True,\n",
    "\n",
    "    # 优化器与混合精度\n",
    "    optim=\"adamw_torch_fused\",    # 新版 PyTorch/Transformers 支持的 fused AdamW\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    fp16=(not torch.cuda.is_bf16_supported()),\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "# 过滤掉旧版本不支持的键\n",
    "train_args = TrainingArguments(**{k:v for k,v in args_dict.items() if k in allowed})\n",
    "\n",
    "acc = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    if isinstance(logits, (tuple, list)):  # 兼容部分版本\n",
    "        logits = logits[0]\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": acc.compute(predictions=preds, references=labels)[\"accuracy\"]}\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_valid,   # 只在手动 evaluate 时用\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# --- 10) 训练前/后各评一次（只打印到控制台，不落盘） ---\n",
    "print(f\"[device] cuda:{torch.cuda.get_device_name(0)} | TF32={torch.backends.cuda.matmul.allow_tf32}\")\n",
    "print(\"=== Eval BEFORE fine-tune ===\")\n",
    "pre_metrics = trainer.evaluate()\n",
    "print(pre_metrics)\n",
    "\n",
    "print(\"=== Training (no checkpoints / no mid-eval) ===\")\n",
    "t0 = time.time()\n",
    "train_out = trainer.train()\n",
    "print(f\"train_time = {time.time() - t0:.1f}s ; steps = {train_out.global_step}\")\n",
    "\n",
    "print(\"=== Eval AFTER fine-tune ===\")\n",
    "post_metrics = trainer.evaluate()\n",
    "print(post_metrics)\n",
    "\n",
    "# （如需保存最终权重可手动解开，但会产生一次 I/O）\n",
    "#trainer.save_model(os.path.join(ramdisk, \"_out\", \"final\"))\n",
    "\n",
    "pred = trainer.predict(hf_test)\n",
    "logits = pred.predictions[0] if isinstance(pred.predictions, (tuple, list)) else pred.predictions\n",
    "labels = logits.argmax(axis=-1)\n",
    "\n",
    "\n",
    "out_csv = \"./outputs/submission_bert2.csv\"\n",
    "pd.DataFrame({\"sentiments\": labels}).to_csv(out_csv, index=False)\n",
    "print(\"wrote:\", out_csv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee6483",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
