{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461fdfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= server_min_io_finetune.py =======\n",
    "\n",
    "import os, pathlib\n",
    "\n",
    "# --- 1) 强制单卡 / 清理分布式变量，避免误走多卡 & P2P 映射 ---\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "for k in (\"RANK\",\"LOCAL_RANK\",\"WORLD_SIZE\",\"MASTER_ADDR\",\"MASTER_PORT\"):\n",
    "    os.environ.pop(k, None)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# --- 2) Hugging Face 缓存尽量走内存盘/本地快盘，减少慢盘 I/O ---\n",
    "ramdisk = \"/dev/shm\" if os.path.isdir(\"/dev/shm\") else \".\"\n",
    "hf_home = os.path.join(ramdisk, \".hf_cache\")\n",
    "os.makedirs(hf_home, exist_ok=True)\n",
    "os.environ[\"HF_HOME\"] = hf_home\n",
    "os.environ[\"HF_HUB_CACHE\"] = os.path.join(hf_home, \"hub\")\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = os.path.join(hf_home, \"transformers\")\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = os.path.join(hf_home, \"datasets\")\n",
    "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"   # 纯本地，少网络交互\n",
    "\n",
    "# --------------------- 下面才开始 import ---------------------\n",
    "import json, time, random\n",
    "import numpy as np, pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, disable_caching\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments, DataCollatorWithPadding\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# --- 3) CUDA 设备 & TF32（Ampere 加速） ---\n",
    "assert torch.cuda.is_available(), \"CUDA 不可用：确认驱动/容器/权限\"\n",
    "device = torch.device(\"cuda\")\n",
    "torch.set_float32_matmul_precision(\"high\")      # TF32 快路径（2.0+）:contentReference[oaicite:3]{index=3}\n",
    "torch.backends.cuda.matmul.allow_tf32 = True    # Ampere 张量核 TF32 开启:contentReference[oaicite:4]{index=4}\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# --- 4) 随机种子 ---\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# --- 5) 读入你的本地数据（/mnt/data/train.json），自动识别列名 ---\n",
    "def read_json_flexible(path: str) -> pd.DataFrame:\n",
    "    # 先按 JSON Lines 读；不行就回退到普通 JSON\n",
    "    try:\n",
    "        return pd.read_json(path, lines=True)\n",
    "    except ValueError:\n",
    "        return pd.read_json(path)\n",
    "\n",
    "DATA_PATH = \"../train.json\"\n",
    "df = read_json_flexible(DATA_PATH)\n",
    "text_key_candidates  = [\"text\",\"review\",\"reviews\",\"sentence\",\"content\"]\n",
    "label_key_candidates = [\"label\",\"labels\",\"sentiment\",\"sentiments\",\"target\",\"y\"]\n",
    "text_col  = next((c for c in text_key_candidates  if c in df.columns), None)\n",
    "label_col = next((c for c in label_key_candidates if c in df.columns), None)\n",
    "assert text_col and label_col, f\"列名识别失败，请检查字段；现有列：{df.columns.tolist()}\"\n",
    "\n",
    "df = df[[text_col, label_col]].rename(columns={text_col:\"text\", label_col:\"label\"}).copy()\n",
    "# 规范二分类标签到 {0,1}\n",
    "mapping = {\"neg\":0,\"negative\":0,\"0\":0,\"pos\":1,\"positive\":1,\"1\":1}\n",
    "if not pd.api.types.is_integer_dtype(df[\"label\"]):\n",
    "    df[\"label\"] = df[\"label\"].astype(str).str.lower().map(mapping)\n",
    "df = df.dropna(subset=[\"label\"]).astype({\"label\":\"int64\"})\n",
    "\n",
    "# 训练/验证划分（分层）\n",
    "train_df, valid_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=SEED, stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "# --- 6) HF Datasets：禁用磁盘缓存 + 仅驻内存 tokenization ---\n",
    "disable_caching()  # 全局禁用 datasets 缓存（transform 结果不落盘）:contentReference[oaicite:5]{index=5}\n",
    "def to_hf(pdf: pd.DataFrame) -> Dataset:\n",
    "    return Dataset.from_pandas(pdf[[\"text\",\"label\"]], preserve_index=False)\n",
    "\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\", \"distilbert-base-uncased\")  # 轻量快速\n",
    "MAX_LEN = int(os.getenv(\"MAX_LEN\", \"128\"))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "def tok_fn(batch):  # 只做截断，padding 交给 collator\n",
    "    return tokenizer(batch[\"text\"], truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "hf_train = to_hf(train_df).map(tok_fn, batched=True, batch_size=2048,\n",
    "                               remove_columns=[\"text\"], keep_in_memory=True, load_from_cache_file=False)\n",
    "hf_valid = to_hf(valid_df).map(tok_fn, batched=True, batch_size=2048,\n",
    "                               remove_columns=[\"text\"], keep_in_memory=True, load_from_cache_file=False)\n",
    "hf_train = hf_train.rename_column(\"label\", \"labels\").with_format(\"torch\")\n",
    "hf_valid = hf_valid.rename_column(\"label\", \"labels\").with_format(\"torch\")\n",
    "\n",
    "# --- 7) 模型 ---\n",
    "id2label = {0:\"NEG\", 1:\"POS\"}; label2id = {\"NEG\":0, \"POS\":1}\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=2, id2label=id2label, label2id=label2id, cache_dir=os.environ[\"TRANSFORMERS_CACHE\"]\n",
    ").to(device)\n",
    "\n",
    "# --- 8) Collator：按 8 对齐 padding，利于张量核 ---\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)  # 文档支持该参数:contentReference[oaicite:6]{index=6}\n",
    "\n",
    "# --- 9) 最小 I/O 的 TrainingArguments（不存盘、不报告、不评估中间结果） ---\n",
    "from dataclasses import fields as dataclass_fields\n",
    "allowed = {f.name for f in dataclass_fields(TrainingArguments)}\n",
    "\n",
    "args_dict = dict(\n",
    "    output_dir=os.path.join(ramdisk, \"_out\"),  # 若有 /dev/shm 则写内存盘；否则当前目录\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    # ——全程不写盘、仅控制台打印——\n",
    "    save_strategy=\"no\",\n",
    "    load_best_model_at_end=False,\n",
    "    report_to=\"none\",\n",
    "    logging_strategy=\"no\",\n",
    "\n",
    "    # 训练中不评估；我们手动在训练前/后各评一次，避免 I/O\n",
    "    eval_strategy=\"no\" if \"eval_strategy\" in allowed else None,\n",
    "    evaluation_strategy=None if \"eval_strategy\" in allowed else \"no\",\n",
    "\n",
    "    # DataLoader：GPU 更友好（pin_memory/num_workers）\n",
    "    dataloader_pin_memory=True,   # PyTorch 官方推荐 GPU 下开启:contentReference[oaicite:7]{index=7}\n",
    "    dataloader_num_workers=4,     # 典型 2~8；需结合机器核数调优:contentReference[oaicite:8]{index=8}\n",
    "    group_by_length=True,\n",
    "\n",
    "    # 优化器与混合精度\n",
    "    optim=\"adamw_torch_fused\",    # 新版 PyTorch/Transformers 支持的 fused AdamW\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    fp16=(not torch.cuda.is_bf16_supported()),\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "# 过滤掉旧版本不支持的键\n",
    "train_args = TrainingArguments(**{k:v for k,v in args_dict.items() if k in allowed})\n",
    "\n",
    "acc = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    if isinstance(logits, (tuple, list)):  # 兼容部分版本\n",
    "        logits = logits[0]\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": acc.compute(predictions=preds, references=labels)[\"accuracy\"]}\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_valid,   # 只在手动 evaluate 时用\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# --- 10) 训练前/后各评一次（只打印到控制台，不落盘） ---\n",
    "print(f\"[device] cuda:{torch.cuda.get_device_name(0)} | TF32={torch.backends.cuda.matmul.allow_tf32}\")\n",
    "print(\"=== Eval BEFORE fine-tune ===\")\n",
    "pre_metrics = trainer.evaluate()\n",
    "print(pre_metrics)\n",
    "\n",
    "print(\"=== Training (no checkpoints / no mid-eval) ===\")\n",
    "t0 = time.time()\n",
    "train_out = trainer.train()\n",
    "print(f\"train_time = {time.time() - t0:.1f}s ; steps = {train_out.global_step}\")\n",
    "\n",
    "print(\"=== Eval AFTER fine-tune ===\")\n",
    "post_metrics = trainer.evaluate()\n",
    "print(post_metrics)\n",
    "\n",
    "# （如需保存最终权重可手动解开，但会产生一次 I/O）\n",
    "# trainer.save_model(os.path.join(ramdisk, \"_out\", \"final\"))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
